apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: untitled-
  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.19, pipelines.kubeflow.org/pipeline_compilation_time: '2024-09-17T14:07:41.642959',
    pipelines.kubeflow.org/pipeline_spec: '{"name": "untitled"}'}
  labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.19}
spec:
  entrypoint: untitled
  templates:
  - name: chicago-taxi-trips-dataset
    container:
      args: []
      command:
      - sh
      - -c
      - |
        set -e -x -o pipefail
        output_path="$0"
        select="$1"
        where="$2"
        limit="$3"
        format="$4"
        mkdir -p "$(dirname "$output_path")"
        curl --get 'https://data.cityofchicago.org/resource/wrvz-psew.'"${format}"           --data-urlencode '$limit='"${limit}"           --data-urlencode '$where='"${where}"           --data-urlencode '$select='"${select}"           | tr -d '"' > "$output_path"  # Removing unneeded quotes around all numbers
      - /tmp/outputs/Table/data
      - tips,trip_seconds,trip_miles,pickup_community_area,dropoff_community_area,fare,tolls,extras,trip_total
      - trip_start_timestamp >= '2019-01-01' AND trip_start_timestamp < '2019-02-01'
      - '1000'
      - csv
      image: byrnedo/alpine-curl@sha256:548379d0a4a0c08b9e55d9d87a592b7d35d9ab3037f4936f5ccd09d0b625a342
    outputs:
      artifacts:
      - {name: chicago-taxi-trips-dataset-Table, path: /tmp/outputs/Table/data}
    metadata:
      annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/datasets/Chicago_Taxi_Trips/component.yaml',
        pipelines.kubeflow.org/task_display_name: Chicago Taxi Trips dataset, pipelines.kubeflow.org/component_spec: '{"description":
          "City of Chicago Taxi Trips dataset: https://data.cityofchicago.org/Transportation/Taxi-Trips/wrvz-psew\n\nThe
          input parameters configure the SQL query to the database.\nThe dataset is
          pretty big, so limit the number of results using the `Limit` or `Where`
          parameters.\nRead [Socrata dev](https://dev.socrata.com/docs/queries/) for
          the advanced query syntax\n", "implementation": {"container": {"command":
          ["sh", "-c", "set -e -x -o pipefail\noutput_path=\"$0\"\nselect=\"$1\"\nwhere=\"$2\"\nlimit=\"$3\"\nformat=\"$4\"\nmkdir
          -p \"$(dirname \"$output_path\")\"\ncurl --get ''https://data.cityofchicago.org/resource/wrvz-psew.''\"${format}\"           --data-urlencode
          ''$limit=''\"${limit}\"           --data-urlencode ''$where=''\"${where}\"           --data-urlencode
          ''$select=''\"${select}\"           | tr -d ''\"'' > \"$output_path\"  #
          Removing unneeded quotes around all numbers\n", {"outputPath": "Table"},
          {"inputValue": "Select"}, {"inputValue": "Where"}, {"inputValue": "Limit"},
          {"inputValue": "Format"}], "image": "byrnedo/alpine-curl@sha256:548379d0a4a0c08b9e55d9d87a592b7d35d9ab3037f4936f5ccd09d0b625a342"}},
          "inputs": [{"default": "trip_start_timestamp>=\"1900-01-01\" AND trip_start_timestamp<\"2100-01-01\"",
          "name": "Where", "type": "String"}, {"default": "1000", "description": "Number
          of rows to return. The rows are randomly sampled.", "name": "Limit", "type":
          "Integer"}, {"default": "trip_id,taxi_id,trip_start_timestamp,trip_end_timestamp,trip_seconds,trip_miles,pickup_census_tract,dropoff_census_tract,pickup_community_area,dropoff_community_area,fare,tips,tolls,extras,trip_total,payment_type,company,pickup_centroid_latitude,pickup_centroid_longitude,pickup_centroid_location,dropoff_centroid_latitude,dropoff_centroid_longitude,dropoff_centroid_location",
          "name": "Select", "type": "String"}, {"default": "csv", "description": "Output
          data format. Suports csv,tsv,cml,rdf,json", "name": "Format", "type": "String"}],
          "metadata": {"annotations": {"author": "Alexey Volkov <alexey.volkov@ark-kun.com>",
          "canonical_location": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/datasets/Chicago_Taxi_Trips/component.yaml"}},
          "name": "Chicago Taxi Trips dataset", "outputs": [{"description": "Result
          type depends on format. CSV and TSV have header.", "name": "Table"}]}',
        pipelines.kubeflow.org/component_ref: '{"digest": "a6ee608a6f0b2fbca565a567a9ac0fdfcd138073460d344e182c9501c69461af"}',
        pipelines.kubeflow.org/arguments.parameters: '{"Format": "csv", "Limit": "1000",
          "Select": "tips,trip_seconds,trip_miles,pickup_community_area,dropoff_community_area,fare,tolls,extras,trip_total",
          "Where": "trip_start_timestamp >= ''2019-01-01'' AND trip_start_timestamp
          < ''2019-02-01''"}'}
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.19
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
  - name: download-data
    container:
      args: []
      command:
      - sh
      - -exc
      - |
        url="$0"
        output_path="$1"
        curl_options="$2"

        mkdir -p "$(dirname "$output_path")"
        curl --get "$url" --output "$output_path" $curl_options
      - https://raw.githubusercontent.com/sarvex/mobius/main/model.json
      - /tmp/outputs/Data/data
      - --location
      image: byrnedo/alpine-curl@sha256:548379d0a4a0c08b9e55d9d87a592b7d35d9ab3037f4936f5ccd09d0b625a342
    outputs:
      artifacts:
      - {name: download-data-Data, path: /tmp/outputs/Data/data}
    metadata:
      annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, pipelines.kubeflow.org/task_display_name: Download
          data, pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"command": ["sh", "-exc", "url=\"$0\"\noutput_path=\"$1\"\ncurl_options=\"$2\"\n\nmkdir
          -p \"$(dirname \"$output_path\")\"\ncurl --get \"$url\" --output \"$output_path\"
          $curl_options\n", {"inputValue": "Url"}, {"outputPath": "Data"}, {"inputValue":
          "curl options"}], "image": "byrnedo/alpine-curl@sha256:548379d0a4a0c08b9e55d9d87a592b7d35d9ab3037f4936f5ccd09d0b625a342"}},
          "inputs": [{"name": "Url", "type": "URI"}, {"default": "--location", "description":
          "Additional options given to the curl program. See https://curl.haxx.se/docs/manpage.html",
          "name": "curl options", "type": "string"}], "metadata": {"annotations":
          {"author": "Alexey Volkov <alexey.volkov@ark-kun.com>"}}, "name": "Download
          data", "outputs": [{"name": "Data"}]}', pipelines.kubeflow.org/component_ref: '{"digest":
          "e60293cc31189041a088e071d8065471c825a35ec9d3cd8f467ec18d4b9f2257"}', pipelines.kubeflow.org/arguments.parameters: '{"Url":
          "https://raw.githubusercontent.com/sarvex/mobius/main/model.json", "curl
          options": "--location"}'}
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.19
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
  - name: keras-train-classifier-from-csv
    container:
      args: [--training-features, /tmp/inputs/training_features/data, --training-labels,
        /tmp/inputs/training_labels/data, --network-json, /tmp/inputs/network_json/data,
        --loss-name, categorical_crossentropy, --num-classes, '0', --optimizer, rmsprop,
        --optimizer-config, '{"learning_rate": 0.01}', --learning-rate, '0.01', --num-epochs,
        '100', --batch-size, '32', --random-seed, '0', --model, /tmp/outputs/model/data,
        '----output-paths', /tmp/outputs/final_loss/data, /tmp/outputs/final_metrics/data,
        /tmp/outputs/metrics_history/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'keras==3.5.0' 'pandas==2.2.2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
        -m pip install --quiet --no-warn-script-location 'keras==3.5.0' 'pandas==2.2.2'
        --user) && "$0" "$@"
      - python3
      - -u
      - -c
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def keras_train_classifier_from_csv(
            training_features_path,
            training_labels_path,
            network_json_path,
            model_path,
            loss_name = 'categorical_crossentropy',
            num_classes = None,
            optimizer = 'rmsprop',
            optimizer_config = None,
            learning_rate = 0.01,
            num_epochs = 100,
            batch_size = 32,
            random_seed = 0,
        ):
            from pathlib import Path

            import keras
            import numpy
            import pandas
            import tensorflow

            tensorflow.random.set_seed(random_seed)
            numpy.random.seed(random_seed)

            training_features_df = pandas.read_csv(training_features_path).convert_dtypes()
            training_labels_df = pandas.read_csv(training_labels_path).convert_dtypes()

            x_train = training_features_df.to_numpy()
            y_train_labels = training_labels_df.to_numpy()
            print('Training features shape:', x_train.shape)
            print('Numer of training samples:', x_train.shape[0])

            # Convert class vectors to binary class matrices.
            y_train_one_hot = keras.utils.to_categorical(y_train_labels, num_classes)
            x_train = numpy.asarray(x_train).astype(numpy.float32)
            y_train_one_hot = numpy.asarray(y_train_one_hot).astype(numpy.float32)

            model_json_str = Path(network_json_path).read_text()
            model = keras.models.model_from_json(model_json_str)

            model.add(keras.layers.Activation('softmax'))

            # Initializing the optimizer
            optimizer_config = optimizer_config or {}
            optimizer_config['learning_rate'] = learning_rate
            optimizer = keras.optimizers.deserialize({
                'class_name': optimizer,
                'config': optimizer_config,
            })

            model.compile(
                loss=loss_name,
                optimizer=optimizer,
                metrics=['accuracy'],
            )

            history = model.fit(
                x_train,
                y_train_one_hot,
                batch_size=batch_size,
                epochs=num_epochs,
                shuffle=True
            )

            model.save(model_path + '.h5')

            metrics_history = {name: [float(value) for value in values] for name, values in history.history.items()}
            final_metrics = {name: values[-1] for name, values in metrics_history.items()}
            final_loss = final_metrics['loss']
            return (final_loss, final_metrics, metrics_history)

        import json
        def _serialize_float(float_value: float) -> str:
            if isinstance(float_value, str):
                return float_value
            if not isinstance(float_value, (float, int)):
                raise TypeError('Value "{}" has type "{}" instead of float.'.format(str(float_value), str(type(float_value))))
            return str(float_value)

        def _serialize_json(obj) -> str:
            if isinstance(obj, str):
                return obj
            import json
            def default_serializer(obj):
                if hasattr(obj, 'to_struct'):
                    return obj.to_struct()
                else:
                    raise TypeError("Object of type '%s' is not JSON serializable and does not have .to_struct() method." % obj.__class__.__name__)
            return json.dumps(obj, default=default_serializer, sort_keys=True)

        import argparse
        _parser = argparse.ArgumentParser(prog='Keras train classifier from csv', description='Trains classifier model using Keras')
        _parser.add_argument("--training-features", dest="training_features_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--training-labels", dest="training_labels_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--network-json", dest="network_json_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--loss-name", dest="loss_name", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--num-classes", dest="num_classes", type=int, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--optimizer", dest="optimizer", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--optimizer-config", dest="optimizer_config", type=json.loads, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--learning-rate", dest="learning_rate", type=float, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--num-epochs", dest="num_epochs", type=int, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--batch-size", dest="batch_size", type=int, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--random-seed", dest="random_seed", type=int, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--model", dest="model_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=3)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = keras_train_classifier_from_csv(**_parsed_args)

        _output_serializers = [
            _serialize_float,
            _serialize_json,
            _serialize_json,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      image: tensorflow/tensorflow:2.17.0
    inputs:
      artifacts:
      - {name: download-data-Data, path: /tmp/inputs/network_json/data}
      - {name: pandas-feature-dataframe-in-csv-format-transformed_table, path: /tmp/inputs/training_features/data}
      - {name: pandas-label-dataframe-in-csv-format-transformed_table, path: /tmp/inputs/training_labels/data}
    outputs:
      artifacts:
      - {name: keras-train-classifier-from-csv-final_loss, path: /tmp/outputs/final_loss/data}
      - {name: keras-train-classifier-from-csv-final_metrics, path: /tmp/outputs/final_metrics/data}
      - {name: keras-train-classifier-from-csv-metrics_history, path: /tmp/outputs/metrics_history/data}
      - {name: keras-train-classifier-from-csv-model, path: /tmp/outputs/model/data}
    metadata:
      annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/keras/Train_classifier/from_CSV/component.yaml',
        pipelines.kubeflow.org/task_display_name: Keras train classifier from csv,
        pipelines.kubeflow.org/component_spec: '{"description": "Trains classifier
          model using Keras.\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>",
          "implementation": {"container": {"args": ["--training-features", {"inputPath":
          "training_features"}, "--training-labels", {"inputPath": "training_labels"},
          "--network-json", {"inputPath": "network_json"}, {"if": {"cond": {"isPresent":
          "loss_name"}, "then": ["--loss-name", {"inputValue": "loss_name"}]}}, {"if":
          {"cond": {"isPresent": "num_classes"}, "then": ["--num-classes", {"inputValue":
          "num_classes"}]}}, {"if": {"cond": {"isPresent": "optimizer"}, "then": ["--optimizer",
          {"inputValue": "optimizer"}]}}, {"if": {"cond": {"isPresent": "optimizer_config"},
          "then": ["--optimizer-config", {"inputValue": "optimizer_config"}]}}, {"if":
          {"cond": {"isPresent": "learning_rate"}, "then": ["--learning-rate", {"inputValue":
          "learning_rate"}]}}, {"if": {"cond": {"isPresent": "num_epochs"}, "then":
          ["--num-epochs", {"inputValue": "num_epochs"}]}}, {"if": {"cond": {"isPresent":
          "batch_size"}, "then": ["--batch-size", {"inputValue": "batch_size"}]}},
          {"if": {"cond": {"isPresent": "random_seed"}, "then": ["--random-seed",
          {"inputValue": "random_seed"}]}}, "--model", {"outputPath": "model"}, "----output-paths",
          {"outputPath": "final_loss"}, {"outputPath": "final_metrics"}, {"outputPath":
          "metrics_history"}], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''keras==3.5.0''
          ''pandas==2.2.2'' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
          --quiet --no-warn-script-location ''keras==3.5.0'' ''pandas==2.2.2'' --user)
          && \"$0\" \"$@\"", "python3", "-u", "-c", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef keras_train_classifier_from_csv(\n    training_features_path,\n    training_labels_path,\n    network_json_path,\n    model_path,\n    loss_name
          = ''categorical_crossentropy'',\n    num_classes = None,\n    optimizer
          = ''rmsprop'',\n    optimizer_config = None,\n    learning_rate = 0.01,\n    num_epochs
          = 100,\n    batch_size = 32,\n    random_seed = 0,\n):\n    from pathlib
          import Path\n\n    import keras\n    import numpy\n    import pandas\n    import
          tensorflow\n\n    tensorflow.random.set_seed(random_seed)\n    numpy.random.seed(random_seed)\n\n    training_features_df
          = pandas.read_csv(training_features_path).convert_dtypes()\n    training_labels_df
          = pandas.read_csv(training_labels_path).convert_dtypes()\n\n    x_train
          = training_features_df.to_numpy()\n    y_train_labels = training_labels_df.to_numpy()\n    print(''Training
          features shape:'', x_train.shape)\n    print(''Numer of training samples:'',
          x_train.shape[0])\n\n    # Convert class vectors to binary class matrices.\n    y_train_one_hot
          = keras.utils.to_categorical(y_train_labels, num_classes)\n    x_train =
          numpy.asarray(x_train).astype(numpy.float32)\n    y_train_one_hot = numpy.asarray(y_train_one_hot).astype(numpy.float32)\n\n    model_json_str
          = Path(network_json_path).read_text()\n    model = keras.models.model_from_json(model_json_str)\n\n    model.add(keras.layers.Activation(''softmax''))\n\n    #
          Initializing the optimizer\n    optimizer_config = optimizer_config or {}\n    optimizer_config[''learning_rate'']
          = learning_rate\n    optimizer = keras.optimizers.deserialize({\n        ''class_name'':
          optimizer,\n        ''config'': optimizer_config,\n    })\n\n    model.compile(\n        loss=loss_name,\n        optimizer=optimizer,\n        metrics=[''accuracy''],\n    )\n\n    history
          = model.fit(\n        x_train,\n        y_train_one_hot,\n        batch_size=batch_size,\n        epochs=num_epochs,\n        shuffle=True\n    )\n\n    model.save(model_path
          + ''.h5'')\n\n    metrics_history = {name: [float(value) for value in values]
          for name, values in history.history.items()}\n    final_metrics = {name:
          values[-1] for name, values in metrics_history.items()}\n    final_loss
          = final_metrics[''loss'']\n    return (final_loss, final_metrics, metrics_history)\n\nimport
          json\ndef _serialize_float(float_value: float) -> str:\n    if isinstance(float_value,
          str):\n        return float_value\n    if not isinstance(float_value, (float,
          int)):\n        raise TypeError(''Value \"{}\" has type \"{}\" instead of
          float.''.format(str(float_value), str(type(float_value))))\n    return str(float_value)\n\ndef
          _serialize_json(obj) -> str:\n    if isinstance(obj, str):\n        return
          obj\n    import json\n    def default_serializer(obj):\n        if hasattr(obj,
          ''to_struct''):\n            return obj.to_struct()\n        else:\n            raise
          TypeError(\"Object of type ''%s'' is not JSON serializable and does not
          have .to_struct() method.\" % obj.__class__.__name__)\n    return json.dumps(obj,
          default=default_serializer, sort_keys=True)\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Keras train classifier from csv'', description=''Trains
          classifier model using Keras'')\n_parser.add_argument(\"--training-features\",
          dest=\"training_features_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--training-labels\",
          dest=\"training_labels_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--network-json\",
          dest=\"network_json_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--loss-name\",
          dest=\"loss_name\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--num-classes\",
          dest=\"num_classes\", type=int, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--optimizer\",
          dest=\"optimizer\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--optimizer-config\",
          dest=\"optimizer_config\", type=json.loads, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--learning-rate\",
          dest=\"learning_rate\", type=float, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--num-epochs\",
          dest=\"num_epochs\", type=int, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--batch-size\",
          dest=\"batch_size\", type=int, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--random-seed\",
          dest=\"random_seed\", type=int, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--model\",
          dest=\"model_path\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\", dest=\"_output_paths\",
          type=str, nargs=3)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = keras_train_classifier_from_csv(**_parsed_args)\n\n_output_serializers
          = [\n    _serialize_float,\n    _serialize_json,\n    _serialize_json,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "tensorflow/tensorflow:2.17.0"}}, "inputs": [{"name": "training_features",
          "optional": true, "type": "CSV"}, {"name": "training_labels", "optional":
          true, "type": "CSV"}, {"name": "network_json", "optional": true, "type":
          "KerasModelJson"}, {"default": "categorical_crossentropy", "name": "loss_name",
          "optional": true, "type": "String"}, {"name": "num_classes", "optional":
          true, "type": "Integer"}, {"default": "rmsprop", "name": "optimizer", "optional":
          true, "type": "String"}, {"name": "optimizer_config", "optional": true,
          "type": "JsonObject"}, {"default": "0.01", "name": "learning_rate", "optional":
          true, "type": "Float"}, {"default": "100", "name": "num_epochs", "optional":
          true, "type": "Integer"}, {"default": "32", "name": "batch_size", "optional":
          true, "type": "Integer"}, {"default": "0", "name": "random_seed", "optional":
          true, "type": "Integer"}], "metadata": {"annotations": {"author": "Alexey
          Volkov <alexey.volkov@ark-kun.com>", "canonical_location": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/keras/Train_classifier/from_CSV/component.yaml"}},
          "name": "Keras train classifier from csv", "outputs": [{"name": "model",
          "type": "KerasModelHdf5"}, {"name": "final_loss", "type": "Float"}, {"name":
          "final_metrics", "type": "JsonObject"}, {"name": "metrics_history", "type":
          "JsonObject"}]}', pipelines.kubeflow.org/component_ref: '{"digest": "82cf40352b19a34ef283314dea445ffaba184c82359e5fcbfabe3a8aa483c5a4"}',
        pipelines.kubeflow.org/arguments.parameters: '{"batch_size": "32", "learning_rate":
          "0.01", "loss_name": "categorical_crossentropy", "num_classes": "0", "num_epochs":
          "100", "optimizer": "rmsprop", "optimizer_config": "{\"learning_rate\":
          0.01}", "random_seed": "0"}'}
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.19
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
  - name: pandas-feature-dataframe-in-csv-format
    container:
      args: [--table, /tmp/inputs/table/data, --transformed-table, /tmp/outputs/transformed_table/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'pandas==2.2.2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
        --quiet --no-warn-script-location 'pandas==2.2.2' --user) && "$0" "$@"
      - python3
      - -u
      - -c
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def Pandas_Transform_DataFrame_in_CSV_format(
            table_path,
            transformed_table_path,
        ):
            import pandas

            df = pandas.read_csv(
                table_path, on_bad_lines='skip'
            )

            df = df.drop(columns=["was_tipped"])
            df.to_csv(
                transformed_table_path,
                index=False,
            )

        import argparse
        _parser = argparse.ArgumentParser(prog='Pandas Feature DataFrame in CSV format', description='Feature')
        _parser.add_argument("--table", dest="table_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--transformed-table", dest="transformed_table_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = Pandas_Transform_DataFrame_in_CSV_format(**_parsed_args)
      image: python:3.9
    inputs:
      artifacts:
      - {name: pandas-load-dataframe-in-csv-format-transformed_table, path: /tmp/inputs/table/data}
    outputs:
      artifacts:
      - {name: pandas-feature-dataframe-in-csv-format-transformed_table, path: /tmp/outputs/transformed_table/data}
    metadata:
      annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/pandas/Transform_DataFrame/in_CSV_format/component.yaml',
        pipelines.kubeflow.org/task_display_name: Pandas Feature DataFrame in CSV
          format, pipelines.kubeflow.org/component_spec: '{"description": "Feature
          DataFrame loaded from a CSV file.\n\n    Inputs:\n        table: Table to
          transform.\n\n    Outputs:\n        transformed_table: Transformed table.\n\n    Annotations:\n        author:
          Alexey Volkov <alexey.volkov@ark-kun.com>", "implementation": {"container":
          {"args": ["--table", {"inputPath": "table"}, "--transformed-table", {"outputPath":
          "transformed_table"}], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''pandas==2.2.2''
          || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          ''pandas==2.2.2'' --user) && \"$0\" \"$@\"", "python3", "-u", "-c", "def
          _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef Pandas_Transform_DataFrame_in_CSV_format(\n    table_path,\n    transformed_table_path,\n):\n    import
          pandas\n\n    df = pandas.read_csv(\n        table_path, on_bad_lines=''skip''\n    )\n\n    df
          = df.drop(columns=[\"was_tipped\"])\n    df.to_csv(\n        transformed_table_path,\n        index=False,\n    )\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Pandas Feature DataFrame
          in CSV format'', description=''Feature'')\n_parser.add_argument(\"--table\",
          dest=\"table_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--transformed-table\",
          dest=\"transformed_table_path\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = Pandas_Transform_DataFrame_in_CSV_format(**_parsed_args)\n"], "image":
          "python:3.9"}}, "inputs": [{"name": "table", "type": "CSV"}], "metadata":
          {"annotations": {"author": "Alexey Volkov <alexey.volkov@ark-kun.com>",
          "canonical_location": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/pandas/Transform_DataFrame/in_CSV_format/component.yaml"}},
          "name": "Pandas Feature DataFrame in CSV format", "outputs": [{"name": "transformed_table",
          "type": "CSV"}]}', pipelines.kubeflow.org/component_ref: '{"digest": "298b58c31a6f3b52226e2d733be0173e396ce63584915d12ab11ce61182076de"}'}
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.19
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
  - name: pandas-label-dataframe-in-csv-format
    container:
      args: [--table, /tmp/inputs/table/data, --transformed-table, /tmp/outputs/transformed_table/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'pandas==2.2.2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
        --quiet --no-warn-script-location 'pandas==2.2.2' --user) && "$0" "$@"
      - python3
      - -u
      - -c
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def Pandas_Transform_DataFrame_in_CSV_format(
            table_path,
            transformed_table_path,
        ):
            import pandas

            df = pandas.read_csv(
                table_path, on_bad_lines='skip'
            )

            df = df["was_tipped"] * 1
            df.to_csv(
                transformed_table_path,
                index=False,
            )

        import argparse
        _parser = argparse.ArgumentParser(prog='Pandas Label DataFrame in CSV format', description='Label')
        _parser.add_argument("--table", dest="table_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--transformed-table", dest="transformed_table_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = Pandas_Transform_DataFrame_in_CSV_format(**_parsed_args)
      image: python:3.9
    inputs:
      artifacts:
      - {name: pandas-load-dataframe-in-csv-format-transformed_table, path: /tmp/inputs/table/data}
    outputs:
      artifacts:
      - {name: pandas-label-dataframe-in-csv-format-transformed_table, path: /tmp/outputs/transformed_table/data}
    metadata:
      annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/pandas/Transform_DataFrame/in_CSV_format/component.yaml',
        pipelines.kubeflow.org/task_display_name: Pandas Label DataFrame in CSV format,
        pipelines.kubeflow.org/component_spec: '{"description": "Label DataFrame loaded
          from a CSV file.\n\n    Inputs:\n        table: Table to transform.\n\n    Outputs:\n        transformed_table:
          Transformed table.\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>",
          "implementation": {"container": {"args": ["--table", {"inputPath": "table"},
          "--transformed-table", {"outputPath": "transformed_table"}], "command":
          ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
          --no-warn-script-location ''pandas==2.2.2'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''pandas==2.2.2''
          --user) && \"$0\" \"$@\"", "python3", "-u", "-c", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef Pandas_Transform_DataFrame_in_CSV_format(\n    table_path,\n    transformed_table_path,\n):\n    import
          pandas\n\n    df = pandas.read_csv(\n        table_path, on_bad_lines=''skip''\n    )\n\n    df
          = df[\"was_tipped\"] * 1\n    df.to_csv(\n        transformed_table_path,\n        index=False,\n    )\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Pandas Label DataFrame
          in CSV format'', description=''Label'')\n_parser.add_argument(\"--table\",
          dest=\"table_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--transformed-table\",
          dest=\"transformed_table_path\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = Pandas_Transform_DataFrame_in_CSV_format(**_parsed_args)\n"], "image":
          "python:3.9"}}, "inputs": [{"name": "table", "type": "CSV"}], "metadata":
          {"annotations": {"author": "Alexey Volkov <alexey.volkov@ark-kun.com>",
          "canonical_location": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/pandas/Transform_DataFrame/in_CSV_format/component.yaml"}},
          "name": "Pandas Label DataFrame in CSV format", "outputs": [{"name": "transformed_table",
          "type": "CSV"}]}', pipelines.kubeflow.org/component_ref: '{"digest": "3c8978c8abc05f29c5b34fe25709f80943e870d338f034344dc3e74fa02d2b8f"}'}
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.19
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
  - name: pandas-load-dataframe-in-csv-format
    container:
      args: [--table, /tmp/inputs/table/data, --transformed-table, /tmp/outputs/transformed_table/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'pandas==2.2.2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
        --quiet --no-warn-script-location 'pandas==2.2.2' --user) && "$0" "$@"
      - python3
      - -u
      - -c
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def Pandas_Transform_DataFrame_in_CSV_format(
            table_path,
            transformed_table_path,
        ):
            import pandas

            df = pandas.read_csv(
                table_path, on_bad_lines='skip'
            )

            df.insert(0, "was_tipped", df["tips"] > 0); del df["tips"]; df = df.fillna(0)
            df.to_csv(
                transformed_table_path,
                index=False,
            )

        import argparse
        _parser = argparse.ArgumentParser(prog='Pandas Transform DataFrame in CSV format', description='Load')
        _parser.add_argument("--table", dest="table_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--transformed-table", dest="transformed_table_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = Pandas_Transform_DataFrame_in_CSV_format(**_parsed_args)
      image: python:3.9
    inputs:
      artifacts:
      - {name: chicago-taxi-trips-dataset-Table, path: /tmp/inputs/table/data}
    outputs:
      artifacts:
      - {name: pandas-load-dataframe-in-csv-format-transformed_table, path: /tmp/outputs/transformed_table/data}
    metadata:
      annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/pandas/Transform_DataFrame/in_CSV_format/component.yaml',
        pipelines.kubeflow.org/task_display_name: Pandas Load DataFrame in CSV format,
        pipelines.kubeflow.org/component_spec: '{"description": "Load DataFrame from
          a CSV file.\n\n    Inputs:\n        table: Table to transform.\n\n    Outputs:\n        transformed_table:
          Transformed table.\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>",
          "implementation": {"container": {"args": ["--table", {"inputPath": "table"},
          "--transformed-table", {"outputPath": "transformed_table"}], "command":
          ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
          --no-warn-script-location ''pandas==2.2.2'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''pandas==2.2.2''
          --user) && \"$0\" \"$@\"", "python3", "-u", "-c", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef Pandas_Transform_DataFrame_in_CSV_format(\n    table_path,\n    transformed_table_path,\n):\n    import
          pandas\n\n    df = pandas.read_csv(\n        table_path, on_bad_lines=''skip''\n    )\n\n    df.insert(0,
          \"was_tipped\", df[\"tips\"] > 0); del df[\"tips\"]; df = df.fillna(0)\n    df.to_csv(\n        transformed_table_path,\n        index=False,\n    )\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Pandas Transform DataFrame
          in CSV format'', description=''Load'')\n_parser.add_argument(\"--table\",
          dest=\"table_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--transformed-table\",
          dest=\"transformed_table_path\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = Pandas_Transform_DataFrame_in_CSV_format(**_parsed_args)\n"], "image":
          "python:3.9"}}, "inputs": [{"name": "table", "type": "CSV"}], "metadata":
          {"annotations": {"author": "Alexey Volkov <alexey.volkov@ark-kun.com>",
          "canonical_location": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/pandas/Transform_DataFrame/in_CSV_format/component.yaml"}},
          "name": "Pandas Load DataFrame in CSV format", "outputs": [{"name": "transformed_table",
          "type": "CSV"}]}', pipelines.kubeflow.org/component_ref: '{"digest": "408f4d89ec41906b930e27010445a8b4d70d796564a85fae368934034c8652c4"}'}
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.19
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
  - name: untitled
    dag:
      tasks:
      - {name: chicago-taxi-trips-dataset, template: chicago-taxi-trips-dataset}
      - {name: download-data, template: download-data}
      - name: keras-train-classifier-from-csv
        template: keras-train-classifier-from-csv
        dependencies: [download-data, pandas-feature-dataframe-in-csv-format, pandas-label-dataframe-in-csv-format]
        arguments:
          artifacts:
          - {name: download-data-Data, from: '{{tasks.download-data.outputs.artifacts.download-data-Data}}'}
          - {name: pandas-feature-dataframe-in-csv-format-transformed_table, from: '{{tasks.pandas-feature-dataframe-in-csv-format.outputs.artifacts.pandas-feature-dataframe-in-csv-format-transformed_table}}'}
          - {name: pandas-label-dataframe-in-csv-format-transformed_table, from: '{{tasks.pandas-label-dataframe-in-csv-format.outputs.artifacts.pandas-label-dataframe-in-csv-format-transformed_table}}'}
      - name: pandas-feature-dataframe-in-csv-format
        template: pandas-feature-dataframe-in-csv-format
        dependencies: [pandas-load-dataframe-in-csv-format]
        arguments:
          artifacts:
          - {name: pandas-load-dataframe-in-csv-format-transformed_table, from: '{{tasks.pandas-load-dataframe-in-csv-format.outputs.artifacts.pandas-load-dataframe-in-csv-format-transformed_table}}'}
      - name: pandas-label-dataframe-in-csv-format
        template: pandas-label-dataframe-in-csv-format
        dependencies: [pandas-load-dataframe-in-csv-format]
        arguments:
          artifacts:
          - {name: pandas-load-dataframe-in-csv-format-transformed_table, from: '{{tasks.pandas-load-dataframe-in-csv-format.outputs.artifacts.pandas-load-dataframe-in-csv-format-transformed_table}}'}
      - name: pandas-load-dataframe-in-csv-format
        template: pandas-load-dataframe-in-csv-format
        dependencies: [chicago-taxi-trips-dataset]
        arguments:
          artifacts:
          - {name: chicago-taxi-trips-dataset-Table, from: '{{tasks.chicago-taxi-trips-dataset.outputs.artifacts.chicago-taxi-trips-dataset-Table}}'}
  arguments:
    parameters: []
  serviceAccountName: pipeline-runner
